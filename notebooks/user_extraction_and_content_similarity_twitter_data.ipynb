{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to create content similarity based on the data fetched from Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import re\n",
    "import tweepy\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import spacy\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Twitter API secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_api = {\n",
    "     \"bearer_token\": os.getenv(\"BEARER_TOKEN\"),\n",
    "    \"api_key\": os.getenv(\"API_KEY\"),\n",
    "    \"api_secret\": os.getenv(\"API_SECRET\"),\n",
    "    \"access_token\": os.getenv(\"ACCESS_TOKEN\"),\n",
    "    \"access_secret\": os.getenv(\"ACCESS_SECRET\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract relevant users from the clusters in the textClust mongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_users_from_clusters(source_uuid, cluster_id, timestamp):\n",
    "    connection = MongoClient(f\"mongodb://localhost:27017/\")\n",
    "    db = connection.textclustDB\n",
    "    \n",
    "    # Extract all tweets of a cluster from the MongoDB database\n",
    "    textids = db[f\"mc_{source_uuid}\"].find_one(\n",
    "        {\"id\": cluster_id},\n",
    "        sort=[(\"timestamp\", -1)],\n",
    "        projection={\n",
    "            \"_id\": 0,\n",
    "            \"textids\": 1\n",
    "            }\n",
    "    )\n",
    "    \n",
    "    # Extract the relevant users\n",
    "    users = db[f\"texts_{source_uuid}\"].find(\n",
    "        {\n",
    "            \"$and\": [\n",
    "                {\"general.text_id\": {\n",
    "                        \"$in\": textids[\"textids\"]\n",
    "                    }\n",
    "                },\n",
    "                {\"$or\": [\n",
    "                    {\"general.time\": {\n",
    "                        \"$lte\": datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S\")\n",
    "                        }\n",
    "                    },\n",
    "                    {\"general.time\": {\n",
    "                        \"$lte\": timestamp.replace(\"T\", \" \")\n",
    "                        }\n",
    "                    }\n",
    "                ]}\n",
    "            ]\n",
    "        },\n",
    "        sort=[(\"general.time\", -1)],\n",
    "        projection = {\n",
    "            \"_id\": 0,\n",
    "            \"user\": \"$specific.user\"\n",
    "        }\n",
    "    ).limit(5000)\n",
    "    users = pd.DataFrame([user['user'] for user in users])\n",
    "    return users.drop_duplicates([\"id_str\"], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = extract_relevant_users_from_clusters(\"8273444c-abdd-4410-829a-970846ebd00e\", 52525, \"2022-02-25T22:41:49\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach to load the required tweets from the Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that handles the loading of thetweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_tweets_per_user_from_timestamp(user, timestamp):\n",
    "    end_time = timestamp.isoformat() + \"Z\"\n",
    "    start_time = timestamp - datetime.timedelta(days=1)\n",
    "    start_time = start_time.isoformat() + \"Z\"\n",
    "    data = pd.DataFrame(columns=[\"user_screen_name\", \"user_id\", \"id\", \"text\", \"created_at\", \"attachments\", \"public_metrics\", \"referenced_tweets\", \"source\"])\n",
    "    user_tweets = __fetch_all_tweets(user['id_str'], end_time, start_time)\n",
    "    if user_tweets is None:\n",
    "        return\n",
    "    tweets_list = []\n",
    "    for tweet in user_tweets:\n",
    "        referenced_tweets = tweet.referenced_tweets\n",
    "        if referenced_tweets is not None:\n",
    "            referenced_tweets = [{'id': ref.id, 'type':ref.type} for ref in referenced_tweets]\n",
    "        tweets_list.append({\"user_screen_name\": user['screen_name'],\n",
    "                            \"user_id\": user['id_str'],\n",
    "                            \"id\": tweet.id,\n",
    "                            \"text\": tweet.text,\n",
    "                            \"created_at\": tweet.created_at,\n",
    "                            \"attachments\": tweet.attachments,\n",
    "                            \"public_metrics\": tweet.public_metrics,\n",
    "                            \"referenced_tweets\": referenced_tweets,\n",
    "                            \"source\": tweet.source})\n",
    "    temp = pd.DataFrame.from_records(tweets_list)\n",
    "    data = pd.concat([data, temp], ignore_index=True)\n",
    "    del tweets_list\n",
    "    del temp\n",
    "    return data\n",
    "\n",
    "def __fetch_all_tweets(user_id: str, end_time: str, start_time: str, pagination_token: str = None):\n",
    "        client = tweepy.Client(bearer_token=twitter_api[\"bearer_token\"], consumer_key=twitter_api[\"api_key\"], consumer_secret=twitter_api[\"api_secret\"], access_token=twitter_api[\"access_token\"], access_token_secret=twitter_api[\"access_secret\"], wait_on_rate_limit=True)\n",
    "        #if pagination_token is None:\n",
    "        tweets = client.get_users_tweets(id=user_id, end_time=end_time, start_time=start_time, max_results=100, tweet_fields=[\"created_at\", \"attachments\", \"public_metrics\", \"referenced_tweets\", \"source\"])\n",
    "        #else:\n",
    "        #    tweets = client.get_users_tweets(id=user_id, end_time=end_time, start_time=start_time, max_results=20, pagination_token=pagination_token, tweet_fields=[\"created_at\", \"attachments\", \"public_metrics\", \"referenced_tweets\", \"source\"])\n",
    "        #if tweets.meta.get(\"next_token\", None) is not None:\n",
    "        #    next_tweets = __fetch_all_tweets(user_id, end_time, start_time, tweets.meta.get(\"next_token\", None))\n",
    "        #    if next_tweets is not None and tweets.data is not None:\n",
    "        #        tweets.data.extend(next_tweets)\n",
    "        #    return tweets.data\n",
    "        #else:\n",
    "        return tweets.data\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization of twitter data fetching\n",
    "\n",
    "In the initial development of this thesis joblib was used to parallelize the tweet fetching. Nevertheless, in the actual implementation joblib did not work with Celery. Therefore it was exchanged with billiard which is a replacement for the standard library multiprocessing that can work inside a Celery worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.datetime.strptime(\"2022-02-25T22:41:49\", \"%Y-%m-%dT%H:%M:%S\")\n",
    "responses = Parallel(n_jobs=cpu_count())(delayed(extract_last_tweets_per_user_from_timestamp)(user, time) for _, user in tqdm(users[0:3000].iterrows(), total=len(users[0:3000])))\n",
    "\n",
    "tweet = pd.DataFrame(columns=[\"user_screen_name\", \"user_id\", \"id\", \"text\", \"created_at\", \"attachments\", \"public_metrics\", \"referenced_tweets\", \"source\"])\n",
    "for response in responses:\n",
    "    tweet = pd.concat([tweet, response], ignore_index=True)\n",
    "del responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limit of parallelization is the rate limit from twitter. If more than 1500 users are in the cluster the API will eventually block the access for 15 minutes.\n",
    "Before parallelization the operation took ca. 3:30 minutes. Now it only takes ca. 25 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the tweets to a parquet file\n",
    "\n",
    "This avoids to rerun the whole tweet fetching if the kernel crashed for whatever reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.to_parquet(f'./parquet_saves/extracted_tweets_xxxxxx.snappy', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tweets stored to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_parquet('./parquet_saves/extracted_tweets_xxxxxx.snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter users with too few tweets in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = tweets[\"user_screen_name\"].value_counts()\n",
    "tweets = tweets[tweets[\"user_screen_name\"].isin(v.index[v.gt(5)])]\n",
    "tweets.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt timestamp types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets['created_at'] = tweets['created_at'].values.astype('datetime64[m]')\n",
    "tweet = tweets.astype({'created_at': 'datetime64[m]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all tweets of a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_texts = tweet.groupby([\"user_screen_name\"]).agg({'text': ' '.join})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to apply the preprocessing\n",
    "\n",
    "The code is used from the original textClust implementation with some slight adaptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import emoji\n",
    "\n",
    "def create_preprocessed_text(text):\n",
    "    # Lower text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove text wrap\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Remove usernames\n",
    "    text = re.sub('@[^\\s]+', '', text)\n",
    "        \n",
    "    # remove the # in #hashtag\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "\n",
    "    ## remove multi exclamation mark\n",
    "    text = re.sub(r\"(\\!)\\1+\", ' multiExclamation ', text)\n",
    "    \n",
    "    # Initialize Punctuation set\n",
    "    exclude = '’“' + string.punctuation\n",
    "    \n",
    "    # Check char characters to see if they are in punctuation\n",
    "    text = [char for char in text if char not in exclude]\n",
    "    \n",
    "    # Join the characters again to form the string.\n",
    "    text = ''.join(text)\n",
    "\n",
    "    # Remove emojis\n",
    "    text = emoji.get_emoji_regexp().sub(r'', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create lemmatizer based on spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser'])\n",
    "\n",
    "def spacy_tokenizer(doc):\n",
    "    return [x.lemma_ for x in nlp(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the TF-IDF vectors for the users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(preprocessor=create_preprocessed_text, tokenizer=spacy_tokenizer, ngram_range=(1,2), max_features=20000)\n",
    "tf_matrix = tf.fit_transform(grouped_texts['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranform the numpy array to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = pd.DataFrame(tf_matrix.toarray(), index=grouped_texts.index.values)\n",
    "# Remove users that only have 0 vectors as they will result in a cosine similarity of NaN\n",
    "tf_matrix = tf_matrix.loc[~(tf_matrix==0).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create similarity matrix for users with TF-IDF\n",
    "\n",
    "This function was later adapted in the code to use tqdm so that one can see the progress in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "result = pd.DataFrame(squareform(pdist(tf_matrix, metric='cosine')), columns=tf_matrix.index.values, index=tf_matrix.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the scores on the diagonal to be zero because a user should not have a similarity to itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(result.values, 1.0)\n",
    "similarity = 1 - result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create similarity graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_adjacency(similarity)\n",
    "\n",
    "F = G.copy()\n",
    "# Define the filter were edges should be cut\n",
    "threshold = 0.9\n",
    "F.remove_edges_from([(n1, n2) for n1, n2, w in F.edges(data=\"weight\") if w < threshold])\n",
    "F.remove_nodes_from(list(nx.isolates(F)))\n",
    "fig = plt.figure(1, figsize=(30, 20), dpi=60)\n",
    "nx.draw(F, with_labels=True, node_size=1000, font_size=24)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eabb0453c7dc57e3cda88eb3c18a07491dfe4a6a097c369f6ee5831a0349db6d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('Masterarbeit_Code-DnaPFa3Q')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
