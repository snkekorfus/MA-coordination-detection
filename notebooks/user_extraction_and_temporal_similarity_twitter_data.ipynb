{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to create temporal similarity based on the data fetched from Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymongo import MongoClient\n",
    "import tweepy\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from dtaidistance import dtw\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Twitter API secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_api = {\n",
    "     \"bearer_token\": os.getenv(\"BEARER_TOKEN\"),\n",
    "    \"api_key\": os.getenv(\"API_KEY\"),\n",
    "    \"api_secret\": os.getenv(\"API_SECRET\"),\n",
    "    \"access_token\": os.getenv(\"ACCESS_TOKEN\"),\n",
    "    \"access_secret\": os.getenv(\"ACCESS_SECRET\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract relevant users from the clusters in the textClust mongoDB database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_users_from_clusters(source_uuid, cluster_id, timestamp):\n",
    "    connection = MongoClient(f\"mongodb://localhost:27017/\")\n",
    "    db = connection.textclustDB\n",
    "    \n",
    "    # Extract all tweets of a cluster from the MongoDB database\n",
    "    textids = db[f\"mc_{source_uuid}\"].find_one(\n",
    "        {\"id\": cluster_id},\n",
    "        sort=[(\"timestamp\", -1)],\n",
    "        projection={\n",
    "            \"_id\": 0,\n",
    "            \"textids\": 1\n",
    "            }\n",
    "    )\n",
    "    \n",
    "    # Extract the relevant users\n",
    "    users = db[f\"texts_{source_uuid}\"].find(\n",
    "        {\n",
    "            \"$and\": [\n",
    "                {\"general.text_id\": {\n",
    "                        \"$in\": textids[\"textids\"]\n",
    "                    }\n",
    "                },\n",
    "                {\"$or\": [\n",
    "                    {\"general.time\": {\n",
    "                        \"$lte\": datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S\")\n",
    "                        }\n",
    "                    },\n",
    "                    {\"general.time\": {\n",
    "                        \"$lte\": timestamp.replace(\"T\", \" \")\n",
    "                        }\n",
    "                    }\n",
    "                ]}\n",
    "            ]\n",
    "        },\n",
    "        sort=[(\"general.time\", -1)],\n",
    "        projection = {\n",
    "            \"_id\": 0,\n",
    "            \"user\": \"$specific.user\"\n",
    "        }\n",
    "    ).limit(5000)\n",
    "    users = pd.DataFrame([user['user'] for user in users])\n",
    "    return users.drop_duplicates([\"id_str\"], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = extract_relevant_users_from_clusters(\"8273444c-abdd-4410-829a-970846ebd00e\", 52525, \"2022-02-25T22:41:49\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach to load the required tweets from the Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that handles the loading of thetweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_last_tweets_per_user_from_timestamp(user, timestamp):\n",
    "    end_time = timestamp.isoformat() + \"Z\"\n",
    "    start_time = timestamp - datetime.timedelta(days=1)\n",
    "    start_time = start_time.isoformat() + \"Z\"\n",
    "    data = pd.DataFrame(columns=[\"user_screen_name\", \"user_id\", \"id\", \"text\", \"created_at\", \"attachments\", \"public_metrics\", \"referenced_tweets\", \"source\"])\n",
    "    user_tweets = __fetch_all_tweets(user['id_str'], end_time, start_time)\n",
    "    if user_tweets is None:\n",
    "        return\n",
    "    tweets_list = []\n",
    "    for tweet in user_tweets:\n",
    "        referenced_tweets = tweet.referenced_tweets\n",
    "        if referenced_tweets is not None:\n",
    "            referenced_tweets = [{'id': ref.id, 'type':ref.type} for ref in referenced_tweets]\n",
    "        tweets_list.append({\"user_screen_name\": user['screen_name'],\n",
    "                            \"user_id\": user['id_str'],\n",
    "                            \"id\": tweet.id,\n",
    "                            \"text\": tweet.text,\n",
    "                            \"created_at\": tweet.created_at,\n",
    "                            \"attachments\": tweet.attachments,\n",
    "                            \"public_metrics\": tweet.public_metrics,\n",
    "                            \"referenced_tweets\": referenced_tweets,\n",
    "                            \"source\": tweet.source})\n",
    "    temp = pd.DataFrame.from_records(tweets_list)\n",
    "    data = pd.concat([data, temp], ignore_index=True)\n",
    "    del tweets_list\n",
    "    del temp\n",
    "    return data\n",
    "\n",
    "def __fetch_all_tweets(user_id: str, end_time: str, start_time: str, pagination_token: str = None):\n",
    "        client = tweepy.Client(bearer_token=twitter_api[\"bearer_token\"], consumer_key=twitter_api[\"api_key\"], consumer_secret=twitter_api[\"api_secret\"], access_token=twitter_api[\"access_token\"], access_token_secret=twitter_api[\"access_secret\"], wait_on_rate_limit=True)\n",
    "        #if pagination_token is None:\n",
    "        tweets = client.get_users_tweets(id=user_id, end_time=end_time, start_time=start_time, max_results=100, tweet_fields=[\"created_at\", \"attachments\", \"public_metrics\", \"referenced_tweets\", \"source\"])\n",
    "        #else:\n",
    "        #    tweets = client.get_users_tweets(id=user_id, end_time=end_time, start_time=start_time, max_results=20, pagination_token=pagination_token, tweet_fields=[\"created_at\", \"attachments\", \"public_metrics\", \"referenced_tweets\", \"source\"])\n",
    "        #if tweets.meta.get(\"next_token\", None) is not None:\n",
    "        #    next_tweets = __fetch_all_tweets(user_id, end_time, start_time, tweets.meta.get(\"next_token\", None))\n",
    "        #    if next_tweets is not None and tweets.data is not None:\n",
    "        #        tweets.data.extend(next_tweets)\n",
    "        #    return tweets.data\n",
    "        #else:\n",
    "        return tweets.data\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization of twitter data fetching\n",
    "\n",
    "In the initial development of this thesis joblib was used to parallelize the tweet fetching. Nevertheless, in the actual implementation joblib did not work with Celery. Therefore it was exchanged with billiard which is a replacement for the standard library multiprocessing that can work inside a Celery worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = datetime.datetime.strptime(\"2022-02-25T22:41:49\", \"%Y-%m-%dT%H:%M:%S\")\n",
    "responses = Parallel(n_jobs=cpu_count())(delayed(extract_last_tweets_per_user_from_timestamp)(user, time) for _, user in tqdm(users[0:3000].iterrows(), total=len(users[0:3000])))\n",
    "\n",
    "tweet = pd.DataFrame(columns=[\"user_screen_name\", \"user_id\", \"id\", \"text\", \"created_at\", \"attachments\", \"public_metrics\", \"referenced_tweets\", \"source\"])\n",
    "for response in responses:\n",
    "    tweet = pd.concat([tweet, response], ignore_index=True)\n",
    "del responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The limit of parallelization is the rate limit from twitter. If more than 1500 users are in the cluster the API will eventually block the access for 15 minutes.\n",
    "Before parallelization the operation took ca. 3:30 minutes. Now it only takes ca. 25 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the tweets to a parquet file\n",
    "\n",
    "This avoids to rerun the whole tweet fetching if the kernel crashed for whatever reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet.to_parquet(f'./parquet_saves/extracted_tweets_xxxxxx.snappy', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tweets stored to a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_parquet('./parquet_saves/extracted_tweets_xxxxxx.snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapt timestamp types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets['created_at'] = tweets['created_at'].values.astype('datetime64[m]')\n",
    "tweet = tweets.astype({'created_at': 'datetime64[m]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create 24 hour time frame based on the date of the newest tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = tweets[\"created_at\"].max()\n",
    "end = start - datetime.timedelta(days=1)\n",
    "tweets = tweets[tweets[\"created_at\"] > end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataframe representing the user behavior as a time-series with 1 minute steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tweet_time_series_for_user(user_df, end_timestamp, start_timestamp):\n",
    "    end = end_timestamp.replace(second=0, microsecond=0)\n",
    "    start = start_timestamp.replace(second=0, microsecond=0)\n",
    "    date_ranges = pd.date_range(start=start, end=end, freq='1min')\n",
    "    bins = pd.cut(user_df['created_at'], bins=date_ranges, right=False, labels=[x for x in range(0,len(date_ranges)-1)])\n",
    "    groups = user_df.groupby(['user_screen_name', bins])\n",
    "    return groups.size().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = create_tweet_time_series_for_user(tweets, tweets[\"created_at\"].max(), tweets[\"created_at\"].min())\n",
    "\n",
    "# Filter users with less than 10 tweets in a timespan of 1 day\n",
    "# This is done because users with only a few tweets will have a low distance\n",
    "# to other users as their are not many warping operations needed\n",
    "time_series = time_series[time_series.sum(axis=1) > 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create similarity matrix for users with DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dtw_distance(x, y):\n",
    "    distance = dtw.distance(x.astype('double'), y.astype('double'), window=2, use_c=True)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(squareform(pdist(time_series, metric=calculate_dtw_distance)), columns=time_series.index.values, index=time_series.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform distances into similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(result.values.flatten())\n",
    "np.fill_diagonal(result.values, max(result.values.flatten()))\n",
    "similarity = 1 - result / max(result.values.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the timeseries of users in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.loc['XXX'].plot(figsize=(8,5), xlabel=\"Time bins in minutes\", fontsize=14)\n",
    "plt.xlabel('Time bins in minutes', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.loc['XXX'].plot(figsize=(8,5), xlabel=\"Time bins in minutes\", fontsize=14)\n",
    "plt.xlabel('Time bins in minutes', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create similarity graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_pandas_adjacency(similarity)\n",
    "\n",
    "F = G.copy()\n",
    "threshold = 0.9\n",
    "F.remove_edges_from([(n1, n2) for n1, n2, w in F.edges(data=\"weight\") if w < threshold])\n",
    "F.remove_nodes_from(list(nx.isolates(F)))\n",
    "fig = plt.figure(1, figsize=(30, 20), dpi=60)\n",
    "nx.draw(F, with_labels=True, node_size=1000, font_size=24)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample random edge\n",
    "\n",
    "This is good for getting two connected users in the graph to inspect their profiles manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.sample(F.edges(), 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eabb0453c7dc57e3cda88eb3c18a07491dfe4a6a097c369f6ee5831a0349db6d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('Masterarbeit_Code-DnaPFa3Q': virtualenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
